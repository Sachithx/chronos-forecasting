{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29615d15bfc740de911f26dc55f64376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/44.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac08165f7da4a2a9d6a7a300d46e539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/65.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2e5c16d8d444ab03244819f91983a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"m4_daily\", split=\"train\")\n",
    "ds.set_format(\"numpy\")  # sequences returned as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'timestamp', 'target', 'category'],\n",
      "    num_rows: 4227\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# check the \"autogluon/chronos_datasets\" dataset\n",
    "print(ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                          timestamp  \\\n",
      "0  T000000  [1994-03-01T12:00:00.000, 1994-03-02T12:00:00....   \n",
      "1  T000001  [1995-02-01T12:00:00.000, 1995-02-02T12:00:00....   \n",
      "2  T000002  [2006-05-01T12:00:00.000, 2006-05-02T12:00:00....   \n",
      "3  T000003  [1996-05-30T12:00:00.000, 1996-05-31T12:00:00....   \n",
      "4  T000004  [1997-03-01T12:00:00.000, 1997-03-02T12:00:00....   \n",
      "\n",
      "                                              target category  \n",
      "0  [1017.1, 1019.3, 1017.0, 1019.2, 1018.7, 1015....    Macro  \n",
      "1  [2793.7, 2793.8, 2803.7, 2805.8, 2802.3, 2795....    Macro  \n",
      "2  [1091.3, 1088.5, 1085.7, 1082.9, 1080.1, 1077....    Macro  \n",
      "3  [1092.0, 1078.0, 1064.0, 1050.0, 1036.0, 1022....    Macro  \n",
      "4  [2938.63, 2956.44, 2964.41, 2972.41, 3014.97, ...    Macro  \n"
     ]
    }
   ],
   "source": [
    "#convert to pandas\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ds[:5])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import List, Iterator, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "from typer_config import use_yaml_config\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import IterableDataset, get_worker_info\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    T5Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import gluonts\n",
    "from gluonts.dataset.common import FileDataset\n",
    "from gluonts.itertools import Cyclic, Map, Filter\n",
    "from gluonts.transform import (\n",
    "    FilterTransformation,\n",
    "    TestSplitSampler,\n",
    "    ValidationSplitSampler,\n",
    "    InstanceSplitter,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    MissingValueImputation,\n",
    "    LeavesMissingValues,\n",
    "    LastValueImputation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chronos import ChronosConfig, ChronosTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = typer.Typer(pretty_exceptions_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    Check if we're on the main process.\n",
    "    \"\"\"\n",
    "    if not dist.is_torchelastic_launched():\n",
    "        return True\n",
    "    return int(os.environ[\"RANK\"]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_on_main(msg: str, logger: logging.Logger, log_level: int = logging.INFO):\n",
    "    \"\"\"\n",
    "    Log the given message using the given logger, if we're on the main process.\n",
    "    \"\"\"\n",
    "    if is_main_process():\n",
    "        logger.log(log_level, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_job_info() -> Dict:\n",
    "    \"\"\"\n",
    "    Returns info about this training job.\n",
    "    \"\"\"\n",
    "    job_info = {}\n",
    "\n",
    "    # CUDA info\n",
    "    job_info[\"cuda_available\"] = torch.cuda.is_available()\n",
    "    if torch.cuda.is_available():\n",
    "        job_info[\"device_count\"] = torch.cuda.device_count()\n",
    "\n",
    "        job_info[\"device_names\"] = {\n",
    "            idx: torch.cuda.get_device_name(idx)\n",
    "            for idx in range(torch.cuda.device_count())\n",
    "        }\n",
    "        job_info[\"mem_info\"] = {\n",
    "            idx: torch.cuda.mem_get_info(device=idx)\n",
    "            for idx in range(torch.cuda.device_count())\n",
    "        }\n",
    "\n",
    "    # DDP info\n",
    "    job_info[\"torchelastic_launched\"] = dist.is_torchelastic_launched()\n",
    "\n",
    "    if dist.is_torchelastic_launched():\n",
    "        job_info[\"world_size\"] = dist.get_world_size()\n",
    "\n",
    "    # Versions\n",
    "    job_info[\"python_version\"] = sys.version.replace(\"\\n\", \" \")\n",
    "    job_info[\"torch_version\"] = torch.__version__\n",
    "    job_info[\"numpy_version\"] = np.__version__\n",
    "    job_info[\"gluonts_version\"] = gluonts.__version__\n",
    "    job_info[\"transformers_version\"] = transformers.__version__\n",
    "    job_info[\"accelerate_version\"] = accelerate.__version__\n",
    "\n",
    "    return job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuda_available': True,\n",
       " 'device_count': 4,\n",
       " 'device_names': {0: 'NVIDIA RTX A5000',\n",
       "  1: 'NVIDIA RTX A5000',\n",
       "  2: 'NVIDIA RTX A5000',\n",
       "  3: 'NVIDIA RTX A5000'},\n",
       " 'mem_info': {0: (23781113856, 25425608704),\n",
       "  1: (25198460928, 25425608704),\n",
       "  2: (25198460928, 25425608704),\n",
       "  3: (25198460928, 25425608704)},\n",
       " 'torchelastic_launched': False,\n",
       " 'python_version': '3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]',\n",
       " 'torch_version': '2.6.0+cu124',\n",
       " 'numpy_version': '1.26.4',\n",
       " 'gluonts_version': '0.16.0',\n",
       " 'transformers_version': '4.47.1',\n",
       " 'accelerate_version': '0.34.2'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_job_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_info(ckpt_path: Path, training_config: Dict):\n",
    "    \"\"\"\n",
    "    Save info about this training job in a json file for documentation.\n",
    "    \"\"\"\n",
    "    assert ckpt_path.is_dir()\n",
    "    with open(ckpt_path / \"training_info.json\", \"w\") as fp:\n",
    "        json.dump(\n",
    "            {\"training_config\": training_config, \"job_info\": get_training_job_info()},\n",
    "            fp,\n",
    "            indent=4,\n",
    "        )\n",
    "\n",
    "\n",
    "def get_next_path(\n",
    "    base_fname: str,\n",
    "    base_dir: Path,\n",
    "    file_type: str = \"yaml\",\n",
    "    separator: str = \"-\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets the next available path in a directory. For example, if `base_fname=\"results\"`\n",
    "    and `base_dir` has files [\"results-0.yaml\", \"results-1.yaml\"], this function returns\n",
    "    \"results-2.yaml\".\n",
    "    \"\"\"\n",
    "    if file_type == \"\":\n",
    "        # Directory\n",
    "        items = filter(\n",
    "            lambda x: x.is_dir() and re.match(f\"^{base_fname}{separator}\\\\d+$\", x.stem),\n",
    "            base_dir.glob(\"*\"),\n",
    "        )\n",
    "    else:\n",
    "        # File\n",
    "        items = filter(\n",
    "            lambda x: re.match(f\"^{base_fname}{separator}\\\\d+$\", x.stem),\n",
    "            base_dir.glob(f\"*.{file_type}\"),\n",
    "        )\n",
    "    run_nums = list(\n",
    "        map(lambda x: int(x.stem.replace(base_fname + separator, \"\")), items)\n",
    "    ) + [-1]\n",
    "\n",
    "    next_num = max(run_nums) + 1\n",
    "    fname = f\"{base_fname}{separator}{next_num}\" + (\n",
    "        f\".{file_type}\" if file_type != \"\" else \"\"\n",
    "    )\n",
    "\n",
    "    return base_dir / fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_id=\"google/t5-efficient-tiny\",\n",
    "    model_type=\"seq2seq\",\n",
    "    vocab_size=4096,\n",
    "    random_init=False,\n",
    "    tie_embeddings=False,\n",
    "    pad_token_id=0,\n",
    "    eos_token_id=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the specified HuggingFace model, adjusting the vocabulary\n",
    "    size, special token IDs, and initialization options.\n",
    "\n",
    "    This allows to set a model up for training on a new vocabulary\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    assert model_type in [\"seq2seq\", \"causal\"] \n",
    "    AutoModelClass = (\n",
    "        AutoModelForSeq2SeqLM if model_type == \"seq2seq\" else AutoModelForCausalLM\n",
    "    )\n",
    "    if random_init:\n",
    "        log_on_main(\"Using random initialization\", logger)\n",
    "        config = AutoConfig.from_pretrained(model_id)\n",
    "        if isinstance(config, T5Config):\n",
    "            # The default initializer_factor (1.0) in transformers is too large\n",
    "            config.initializer_factor = 0.05\n",
    "        config.tie_word_embeddings = tie_embeddings\n",
    "        model = AutoModelClass.from_config(config)\n",
    "    else:\n",
    "        log_on_main(f\"Using pretrained initialization from {model_id}\", logger)\n",
    "        model = AutoModelClass.from_pretrained(model_id)\n",
    "\n",
    "    model.resize_token_embeddings(vocab_size)\n",
    "\n",
    "    model.config.pad_token_id = model.generation_config.pad_token_id = pad_token_id\n",
    "    model.config.eos_token_id = model.generation_config.eos_token_id = eos_token_id\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 14:01:00,273 - __main__ - INFO - Using pretrained initialization from google/t5-efficient-tiny\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e489170d455e47d4b50e02f10e0c2ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41736e650efe48c8b28d98f560384441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/62.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/s124mdg54_01/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e66e5ac5d874a7c93f9a9ab81d54eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(4096, 256)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(4096, 256)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (o): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 4)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=256, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=256, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (o): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=256, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=256, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(4096, 256)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (o): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 4)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (o): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=256, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=256, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-3): 3 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (o): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (k): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (v): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (o): Linear(in_features=256, out_features=256, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=256, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=256, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/s124mdg54_01/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b12c301d82444b9827dd167113cafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/62.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_enough_observations(\n",
    "    entry: dict, min_length: int = 0, max_missing_prop: float = 1.0\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given entry has enough observations in the ``\"target\"`` attribute.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entry\n",
    "        The data entry (dictionary) to be tested.\n",
    "    min_length\n",
    "        The minimum length the ``\"target\"`` attribute must have.\n",
    "    max_missing_prop\n",
    "        The maximum proportion of missing data allowed in the ``\"target\"``\n",
    "        attribute.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        len(entry[\"target\"]) >= min_length\n",
    "        and np.isnan(entry[\"target\"]).mean() <= max_missing_prop\n",
    "    ):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoShuffledIterableDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Shuffle entries from an iterable by temporarily accumulating them\n",
    "    in an intermediate buffer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dataset\n",
    "        The original iterable object, representing the dataset.\n",
    "    shuffle_buffer_length\n",
    "        Size of the buffer use to shuffle entries from the base dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dataset, shuffle_buffer_length: int = 100) -> None:\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.shuffle_buffer_length = shuffle_buffer_length\n",
    "        self.generator = torch.Generator()\n",
    "\n",
    "    def __iter__(self):\n",
    "        shuffle_buffer = []\n",
    "\n",
    "        for element in self.base_dataset:\n",
    "            shuffle_buffer.append(element)\n",
    "            if len(shuffle_buffer) >= self.shuffle_buffer_length:\n",
    "                idx = torch.randint(\n",
    "                    len(shuffle_buffer), size=(), generator=self.generator\n",
    "                )\n",
    "                yield shuffle_buffer.pop(idx)\n",
    "\n",
    "        while shuffle_buffer:\n",
    "            idx = torch.randint(len(shuffle_buffer), size=(), generator=self.generator)\n",
    "            yield shuffle_buffer.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleMixin:\n",
    "    \"\"\"\n",
    "    Mix-in class that datasets can inherit from to get\n",
    "    shuffling functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def shuffle(self, shuffle_buffer_length: int = 100):\n",
    "        return PseudoShuffledIterableDataset(self, shuffle_buffer_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronosDataset(IterableDataset, ShuffleMixin):\n",
    "    \"\"\"\n",
    "    Dataset wrapper, using a ``ChronosTokenizer`` to turn data from a time series\n",
    "    into a HuggingFace-compatible set of ``input_ids``, ``attention_mask`` and\n",
    "    ``labels``.\n",
    "\n",
    "    Entries from the original datasets are assumed to have a ``\"start\"`` attribute\n",
    "    (of type ``pd.Period``), and a ``\"target\"`` attribute (of type ``np.ndarray``).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets\n",
    "        Datasets containing the original time series data.\n",
    "    probabilities\n",
    "        In training mode, data will be sampled from each of the original datasets\n",
    "        with these probabilities.\n",
    "    tokenizer\n",
    "        Tokenizer to be used to turn sequences of real numbers into token IDs.\n",
    "    context_length\n",
    "        Samples context will be limited to this length.\n",
    "    prediction_length\n",
    "        Samples labels will be limited to this length.\n",
    "    drop_prob\n",
    "        In training mode, observations from a sample will be turned into ``np.nan``,\n",
    "        i.e. turned into missing values, with this probability.\n",
    "    min_past\n",
    "        Data samples will be considered only if there's at least ``min_past``-many\n",
    "        historical observations.\n",
    "    mode\n",
    "        One of ``\"training\"``, ``\"validation\"``, or ``\"test\"``.\n",
    "    np_dtype\n",
    "        Numpy float data type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        datasets: list,\n",
    "        probabilities: List[float],\n",
    "        tokenizer: ChronosTokenizer,\n",
    "        context_length: int = 512,\n",
    "        prediction_length: int = 64,\n",
    "        drop_prob: float = 0.2,\n",
    "        min_past: Optional[int] = None,\n",
    "        model_type: str = \"seq2seq\",\n",
    "        imputation_method: Optional[MissingValueImputation] = None,\n",
    "        mode: str = \"training\",\n",
    "        np_dtype=np.float32,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(probabilities) == len(datasets)\n",
    "        assert mode in (\"training\", \"validation\", \"test\")\n",
    "        assert model_type in (\"seq2seq\", \"causal\")\n",
    "\n",
    "        self.datasets = datasets\n",
    "        self.probabilities = probabilities\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.drop_prob = drop_prob if model_type == \"seq2seq\" else 0.0\n",
    "        self.min_past = min_past or prediction_length\n",
    "        self.model_type = model_type\n",
    "        self.imputation_method = imputation_method or LeavesMissingValues()\n",
    "        self.mode = mode\n",
    "        self.np_dtype = np_dtype\n",
    "\n",
    "    def preprocess_entry(self, entry: dict, mode: str) -> dict:\n",
    "        entry = {f: entry[f] for f in [\"start\", \"target\"]}\n",
    "        entry[\"target\"] = np.asarray(entry[\"target\"], dtype=self.np_dtype)\n",
    "        assert entry[\"target\"].ndim == 1, f\"got {entry['target'].ndim=}, expected 1\"\n",
    "\n",
    "        if self.model_type == \"causal\":\n",
    "            # Causal models do not play nice with missing values, so it is\n",
    "            # recommended to use an imputation method, e.g., LastValueImputation\n",
    "            entry[\"target\"] = self.imputation_method(entry[\"target\"])\n",
    "\n",
    "        if mode == \"training\" and self.drop_prob > 0:\n",
    "            target = entry[\"target\"].copy()\n",
    "            drop_p = np.random.uniform(low=0.0, high=self.drop_prob)\n",
    "            mask = np.random.choice(\n",
    "                [True, False], size=len(target), p=[drop_p, 1 - drop_p]\n",
    "            )\n",
    "            target[mask] = np.nan\n",
    "            entry[\"target\"] = target\n",
    "\n",
    "        return entry\n",
    "\n",
    "    def _create_instance_splitter(self, mode: str):\n",
    "        assert mode in [\"training\", \"test\", \"validation\"]\n",
    "\n",
    "        instance_sampler = {\n",
    "            \"training\": ExpectedNumInstanceSampler(\n",
    "                num_instances=1.0,\n",
    "                min_instances=1,\n",
    "                min_past=self.min_past,\n",
    "                min_future=self.prediction_length,\n",
    "            ),\n",
    "            \"test\": TestSplitSampler(),\n",
    "            \"validation\": ValidationSplitSampler(min_future=self.prediction_length),\n",
    "        }[mode]\n",
    "\n",
    "        return InstanceSplitter(\n",
    "            target_field=\"target\",\n",
    "            is_pad_field=\"is_pad\",\n",
    "            start_field=\"start\",\n",
    "            forecast_start_field=\"forecast_start\",\n",
    "            instance_sampler=instance_sampler,\n",
    "            past_length=self.context_length,\n",
    "            future_length=self.prediction_length,\n",
    "            dummy_value=np.nan,\n",
    "        )\n",
    "\n",
    "    def create_training_data(self, data):\n",
    "        data = Cyclic(data)\n",
    "        split_transform = self._create_instance_splitter(\n",
    "            \"training\"\n",
    "        ) + FilterTransformation(\n",
    "            condition=lambda entry: (~np.isnan(entry[\"past_target\"])).sum() > 0\n",
    "        )\n",
    "        data = split_transform.apply(data, is_train=True)\n",
    "        return data\n",
    "\n",
    "    def create_test_data(self, data):\n",
    "        data = self._create_instance_splitter(\"test\").apply(data, is_train=False)\n",
    "        return data\n",
    "\n",
    "    def create_validation_data(self, data):\n",
    "        data = self._create_instance_splitter(\"validation\").apply(data, is_train=False)\n",
    "        return data\n",
    "\n",
    "    def to_hf_format(self, entry: dict) -> dict:\n",
    "        past_target = torch.tensor(entry[\"past_target\"]).unsqueeze(0)\n",
    "        input_ids, attention_mask, scale = self.tokenizer.context_input_transform(\n",
    "            past_target\n",
    "        )\n",
    "        future_target = torch.tensor(entry[\"future_target\"]).unsqueeze(0)\n",
    "        labels, labels_mask = self.tokenizer.label_input_transform(future_target, scale)\n",
    "        labels[labels_mask == 0] = -100\n",
    "\n",
    "        if self.model_type == \"causal\":\n",
    "            # The InstanceSplitter pads time series on the left to be equal to the\n",
    "            # context_length. However, certain models (e.g., GPT2) with absolute\n",
    "            # position embeddings should not be trained with left padding.\n",
    "            # The following piece of code moves padding from left to right.\n",
    "\n",
    "            assert input_ids.shape[-1] == entry[\"past_is_pad\"].shape[0]\n",
    "\n",
    "            # Find the index where padding starts\n",
    "            pad_start_idx = np.searchsorted(1 - entry[\"past_is_pad\"], 1)\n",
    "            padded_input_ids, obs_input_ids = torch.tensor_split(\n",
    "                input_ids, [pad_start_idx], dim=-1\n",
    "            )\n",
    "            padded_attention_mask, obs_attention_mask = torch.tensor_split(\n",
    "                attention_mask, [pad_start_idx], dim=-1\n",
    "            )\n",
    "\n",
    "            # Move padding to the right\n",
    "            input_ids = torch.cat(\n",
    "                [\n",
    "                    obs_input_ids,\n",
    "                    labels,\n",
    "                    padded_input_ids,\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            attention_mask = torch.cat(\n",
    "                [\n",
    "                    obs_attention_mask,\n",
    "                    labels_mask,\n",
    "                    padded_attention_mask,\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "            # labels for causal models are same as the input_ids.\n",
    "            # Internally transformers shifts the labels by one during training.\n",
    "            labels = input_ids.clone()\n",
    "            input_ids[~attention_mask] = self.tokenizer.config.pad_token_id\n",
    "            labels[~attention_mask] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.squeeze(0),\n",
    "            \"attention_mask\": attention_mask.squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "        }\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        preprocessed_datasets = [\n",
    "            Map(\n",
    "                partial(self.preprocess_entry, mode=self.mode),\n",
    "                dataset,\n",
    "            )\n",
    "            for dataset in self.datasets\n",
    "        ]\n",
    "\n",
    "        if self.mode == \"training\":\n",
    "            iterables = [\n",
    "                self.create_training_data(dataset) for dataset in preprocessed_datasets\n",
    "            ]\n",
    "        elif self.mode == \"test\":\n",
    "            iterables = [\n",
    "                self.create_test_data(dataset) for dataset in preprocessed_datasets\n",
    "            ]\n",
    "        else:\n",
    "            iterables = [\n",
    "                self.create_validation_data(dataset)\n",
    "                for dataset in preprocessed_datasets\n",
    "            ]\n",
    "\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            probs = list(self.probabilities)\n",
    "        else:\n",
    "            worker_id = worker_info.id\n",
    "            num_workers = worker_info.num_workers\n",
    "            iterables = list(itertools.islice(iterables, worker_id, None, num_workers))\n",
    "            probs = list(\n",
    "                itertools.islice(self.probabilities, worker_id, None, num_workers)\n",
    "            )\n",
    "\n",
    "        probs = [prob / sum(probs) for prob in probs]\n",
    "\n",
    "        iterators = list(map(iter, iterables))\n",
    "        if self.mode == \"training\":\n",
    "            while True:\n",
    "                idx = np.random.choice(range(len(iterators)), p=probs)\n",
    "                try:\n",
    "                    yield self.to_hf_format(next(iterators[idx]))\n",
    "                except StopIteration:\n",
    "                    probs[idx] = 0\n",
    "                    if sum(probs) == 0:\n",
    "                        return\n",
    "                    probs = [prob / sum(probs) for prob in probs]\n",
    "        else:\n",
    "            for entry in itertools.chain(*iterators):\n",
    "                yield self.to_hf_format(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.command()\n",
    "@use_yaml_config(param_name=\"config\")\n",
    "def main(\n",
    "    training_data_paths: str,\n",
    "    probability: Optional[str] = None,\n",
    "    context_length: int = 512,\n",
    "    prediction_length: int = 64,\n",
    "    min_past: int = 64,\n",
    "    max_steps: int = 200_000,\n",
    "    save_steps: int = 50_000,\n",
    "    log_steps: int = 500,\n",
    "    per_device_train_batch_size: int = 32,\n",
    "    learning_rate: float = 1e-3,\n",
    "    optim: str = \"adamw_torch_fused\",\n",
    "    shuffle_buffer_length: int = 100,\n",
    "    gradient_accumulation_steps: int = 2,\n",
    "    model_id: str = \"google/t5-efficient-tiny\",\n",
    "    model_type: str = \"seq2seq\",\n",
    "    random_init: bool = False,\n",
    "    tie_embeddings: bool = False,\n",
    "    output_dir: str = \"./output/\",\n",
    "    tf32: bool = True,\n",
    "    torch_compile: bool = True,\n",
    "    tokenizer_class: str = \"MeanScaleUniformBins\",\n",
    "    tokenizer_kwargs: str = \"{'low_limit': -15.0, 'high_limit': 15.0}\",\n",
    "    n_tokens: int = 4096,\n",
    "    n_special_tokens: int = 2,\n",
    "    pad_token_id: int = 0,\n",
    "    eos_token_id: int = 1,\n",
    "    use_eos_token: bool = True,\n",
    "    lr_scheduler_type: str = \"linear\",\n",
    "    warmup_ratio: float = 0.0,\n",
    "    dataloader_num_workers: int = 1,\n",
    "    max_missing_prop: float = 0.9,\n",
    "    num_samples: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 1.0,\n",
    "    seed: Optional[int] = None,\n",
    "):\n",
    "    if tf32 and not (\n",
    "        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "    ):\n",
    "        # TF32 floating point format is available only on NVIDIA GPUs\n",
    "        # with compute capability 8 and above. See link for details.\n",
    "        # https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-8-x\n",
    "        log_on_main(\n",
    "            \"TF32 format is only available on devices with compute capability >= 8. \"\n",
    "            \"Setting tf32 to False.\",\n",
    "            logger,\n",
    "        )\n",
    "        tf32 = False\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32)\n",
    "\n",
    "    log_on_main(f\"Using SEED: {seed}\", logger)\n",
    "    transformers.set_seed(seed=seed)\n",
    "\n",
    "    raw_training_config = deepcopy(locals())\n",
    "    output_dir = Path(output_dir)\n",
    "    training_data_paths = ast.literal_eval(training_data_paths)\n",
    "    assert isinstance(training_data_paths, list)\n",
    "\n",
    "    if isinstance(probability, str):\n",
    "        probability = ast.literal_eval(probability)\n",
    "    elif probability is None:\n",
    "        probability = [1.0 / len(training_data_paths)] * len(training_data_paths)\n",
    "    assert isinstance(probability, list)\n",
    "\n",
    "    assert len(training_data_paths) == len(probability)\n",
    "\n",
    "    if dataloader_num_workers > len(training_data_paths):\n",
    "        log_on_main(\n",
    "            f\"Setting the number of data loader workers to {len(training_data_paths)}, \"\n",
    "            f\"instead of {dataloader_num_workers}.\",\n",
    "            logger,\n",
    "        )\n",
    "        dataloader_num_workers = len(training_data_paths)\n",
    "\n",
    "    if isinstance(tokenizer_kwargs, str):\n",
    "        tokenizer_kwargs = ast.literal_eval(tokenizer_kwargs)\n",
    "    assert isinstance(tokenizer_kwargs, dict)\n",
    "\n",
    "    assert model_type in [\"seq2seq\", \"causal\"]\n",
    "\n",
    "    output_dir = get_next_path(\"run\", base_dir=output_dir, file_type=\"\")\n",
    "\n",
    "    log_on_main(f\"Logging dir: {output_dir}\", logger)\n",
    "    log_on_main(\n",
    "        f\"Loading and filtering {len(training_data_paths)} datasets \"\n",
    "        f\"for training: {training_data_paths}\",\n",
    "        logger,\n",
    "    )\n",
    "\n",
    "    log_on_main(\n",
    "        f\"Mixing probabilities: {probability}\",\n",
    "        logger,\n",
    "    )\n",
    "\n",
    "    train_datasets = [\n",
    "        Filter(\n",
    "            partial(\n",
    "                has_enough_observations,\n",
    "                min_length=min_past + prediction_length,\n",
    "                max_missing_prop=max_missing_prop,\n",
    "            ),\n",
    "            FileDataset(path=Path(data_path), freq=\"h\"),\n",
    "        )\n",
    "        for data_path in training_data_paths\n",
    "    ]\n",
    "\n",
    "    log_on_main(\"Initializing model\", logger)\n",
    "\n",
    "    model = load_model(\n",
    "        model_id=model_id,\n",
    "        model_type=model_type,\n",
    "        vocab_size=n_tokens,\n",
    "        random_init=random_init,\n",
    "        tie_embeddings=tie_embeddings,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "    )\n",
    "\n",
    "    chronos_config = ChronosConfig(\n",
    "        tokenizer_class=tokenizer_class,\n",
    "        tokenizer_kwargs=tokenizer_kwargs,\n",
    "        n_tokens=n_tokens,\n",
    "        n_special_tokens=n_special_tokens,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        use_eos_token=use_eos_token,\n",
    "        model_type=model_type,\n",
    "        context_length=context_length,\n",
    "        prediction_length=prediction_length,\n",
    "        num_samples=num_samples,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    # Add extra items to model config so that it's saved in the ckpt\n",
    "    model.config.chronos_config = chronos_config.__dict__\n",
    "\n",
    "    shuffled_train_dataset = ChronosDataset(\n",
    "        datasets=train_datasets,\n",
    "        probabilities=probability,\n",
    "        tokenizer=chronos_config.create_tokenizer(),\n",
    "        context_length=context_length,\n",
    "        prediction_length=prediction_length,\n",
    "        min_past=min_past,\n",
    "        model_type=model_type,\n",
    "        imputation_method=LastValueImputation() if model_type == \"causal\" else None,\n",
    "        mode=\"training\",\n",
    "    ).shuffle(shuffle_buffer_length=shuffle_buffer_length)\n",
    "\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        optim=optim,\n",
    "        logging_dir=str(output_dir / \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=log_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        max_steps=max_steps,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        dataloader_num_workers=dataloader_num_workers,\n",
    "        tf32=tf32,  # remove this if not using Ampere GPUs (e.g., A100)\n",
    "        torch_compile=torch_compile,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=shuffled_train_dataset,\n",
    "    )\n",
    "    log_on_main(\"Training\", logger)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if is_main_process():\n",
    "        model.save_pretrained(output_dir / \"checkpoint-final\")\n",
    "        save_training_info(\n",
    "            output_dir / \"checkpoint-final\", training_config=raw_training_config\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     logging\u001b[38;5;241m.\u001b[39mbasicConfig(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18;43m__file__\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m      5\u001b[0m     app()\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    logger = logging.getLogger(__file__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
